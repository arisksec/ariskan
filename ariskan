#!/usr/bin/env python3

import requests
import argparse
import sys
import re
import time
import random
from urllib.parse import urljoin, urlparse
import queue
import threading
import math
from collections import Counter

# --- Color Definitions for Terminal Output ---
class Colors:
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    MAGENTA = '\033[95m'
    RED = '\033[91m'
    BOLD_RED = '\033[1;91m' # For Extremely Critical
    ORANGE = '\033[38;5;208m' # For High
    ENDC = '\033[0m'
    DISABLED = ''

# --- WAF Bypass & Evasion Resources ---
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'
]

def print_banner():
    """Prints a branded ASCII art banner for the tool."""
    # This version uses an f-string directly, which is cleaner and avoids the .format() error.
    banner = f"""
{Colors.CYAN}
            _     _
  __ _ _ __(_)___| | ____ _ _ __
 / _` | '__| / __| |/ / _` | '_ \\
| (_| | |  | \\__ \\   < (_| | | | |
 \\__,_|_|  |_|___/_|\\_\\__,_|_| |_|

{Colors.MAGENTA}                 BY @arisksec - v4.0{Colors.ENDC}
    """
    print(banner)

def tech_detect(url, session):
    r"""Detects server technology from HTTP headers."""
    print(f"\n{Colors.YELLOW}[*] Performing Technology Detection on {url}...{Colors.ENDC}")
    try:
        response = session.head(url, timeout=5, allow_redirects=True)
        headers = response.headers
        tech = {
            'Server': headers.get('Server'),
            'X-Powered-By': headers.get('X-Powered-By'),
            'X-AspNet-Version': headers.get('X-AspNet-Version'),
            'Via': headers.get('Via'),
            'X-Varnish': headers.get('X-Varnish'),
        }
        if 'cloudflare' in headers.get('Server', '').lower() or 'cf-ray' in headers:
            tech['WAF'] = 'Cloudflare'

        found_tech = {k: v for k, v in tech.items() if v}
        if found_tech:
            print(f"{Colors.GREEN}[+] Technologies Detected:{Colors.ENDC}")
            for key, value in found_tech.items():
                print(f"  - {key}: {value}")
        else:
            print(f"{Colors.CYAN}[-] No specific technologies identified from headers.{Colors.ENDC}")
    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}[!] Technology detection failed: {e}{Colors.ENDC}", file=sys.stderr)

def enumerate_subdomains(domain, session, timeout, wordlist_path=None):
    r"""Enumerates subdomains using a vastly expanded common list or a custom wordlist."""
    print(f"\n{Colors.YELLOW}[*] Starting Ultra-Deep Subdomain Enumeration for {domain}...{Colors.ENDC}")
    found_subdomains = []

    subs_to_check = []
    if wordlist_path:
        print(f"[*] Using custom wordlist for subdomains: {wordlist_path}")
        try:
            with open(wordlist_path, 'r', encoding='utf-8') as f:
                subs_to_check = [line.strip() for line in f if line.strip()]
        except IOError as e:
            print(f"{Colors.RED}[!] Error reading subdomain wordlist: {e}{Colors.ENDC}", file=sys.stderr)
            return []
    else:
        subs_to_check = [
            'www', 'api', 'dev', 'staging', 'test', 'app', 'blog', 'mail', 'shop', 'cdn', 'assets', 'vpn', 'mfa',
            'sso', 'auth', 'oauth', 'identity', 'login', 'signup', 'support', 'help', 'docs', 'developer', 'status',
            'static', 'media', 'files', 'download', 'upload', 'email', 'mx', 'webmail', 'ftp', 'sftp',
            'db', 'database', 'sql', 'mysql', 'postgres', 'redis', 'mongo', 'admin', 'administrator', 'cpanel',
            'dashboard', 'portal', 'gateway', 'proxy', 'internal', 'private', 'corp', 'intranet', 'extranet',
            'ci', 'cd', 'jenkins', 'gitlab', 'github', 'jira', 'confluence', 'nexus', 'artifactory', 'docker',
            'registry', 'k8s', 'kubernetes', 'swarm', 'aws', 'azure', 'gcp', 'cloud', 'storage', 's3', 'blob',
            'analytics', 'metrics', 'prometheus', 'grafana', 'kibana', 'elasticsearch', 'log', 'graylog',
            'payment', 'billing', 'checkout', 'store', 'market', 'community', 'forum', 'chat', 'voice', 'video',
            'conference', 'meeting', 'calendar', 'drive', 'share', 'transfer', 'backup', 'archive',
            'legacy', 'old', 'v1', 'v2', 'v3', 'next', 'beta', 'alpha', 'demo', 'example', 'sandbox', 'prod',
            'production', 'uat', 'qa', 'preprod', 'customer', 'partner', 'vendor', 'supplier', 'hr', 'finance',
            'marketing', 'sales', 'legal', 'it', 'noc', 'soc', 'security', 'alerts', 'monitoring', 'nagios',
            'zabbix', 'icinga', 'cacti', 'observium', 'librenms', 'smokeping', 'rancid', 'oxidized', 'ansible',
            'puppet', 'chef', 'salt', 'terraform', 'vault', 'consul', 'nomad', 'packer', 'vagrant', 'etcd',
            'coreos', 'rancher', 'openshift', 'mesos', 'marathon', 'chronos', 'dcos', 'minikube', 'microk8s',
            'k3s', 'eks', 'gke', 'aks', 'ecs', 'fargate', 'lambda', 'functions', 'serverless', 'faas', 'paas',
             'iaas', 'saas', 'caas', 'dbass', 'object-storage', 'block-storage', 'file-storage', 'archive-storage',
            'glacier', 'deep-archive', 'efs', 'nfs', 'smb', 'cifs', 'iscsi', 'fc', 'fcoe', 'rdp', 'ssh', 'telnet',
            'vnc', 'http', 'https', 'dns', 'ntp', 'snmp', 'smtp', 'pop3', 'imap', 'ldap', 'kerberos', 'radius',
            'tacacs', 'diameter', 'sip', 'rtp', 'webrtc', 'xmpp', 'mqtt', 'amqp', 'kafka', 'rabbitmq', 'zeromq',
            'activemq', 'pulsar', 'nats', 'redis-sentinel', 'redis-cluster', 'memcached', 'hazelcast', 'ignite',
             'geode', 'cassandra', 'scylla', 'hbase', 'hadoop', 'spark', 'hive', 'presto', 'trino', 'druid',
            'flink', 'storm', 'samza', 'beam', 'airflow', 'luigi', 'argo', 'tekton', 'spinnaker', 'flux',
            'helm', 'kustomize', 'istio', 'linkerd', 'envoy', 'traefik', 'nginx', 'apache', 'httpd', 'tomcat',
            'jboss', 'wildfly', 'glassfish', 'websphere', 'weblogic', 'iis', 'express', 'koa', 'hapi', 'fastify',
            'django', 'flask', 'pyramid', 'tornado', 'rails', 'sinatra', 'hanami', 'laravel', 'symfony', 'codeigniter',
            'yii', 'cakephp', 'zend', 'phalcon', 'lumen', 'slim', 'spring', 'springboot', 'quarkus', 'micronaut',
            'helidon', 'ktor', 'akka', 'play', 'lift', 'scalyr', 'scalog', 'loggly', 'logz', 'sumologic',
            'datadog', 'newrelic', 'appdynamics', 'dynatrace', 'sentry', 'bugsnag', 'rollbar', 'honeycomb',
            'lightstep', 'jaeger', 'zipkin', 'opentracing', 'opentelemetry', 'fluentd', 'fluentbit', 'logstash',
            'filebeat', 'metricbeat', 'packetbeat', 'heartbeat', 'auditbeat', 'journalbeat', 'winlogbeat',
             'osquery', 'wazuh', 'suricata', 'snort', 'bro', 'zeek', 'ossec', 'tripwire', 'aide', 'samhain',
            'clamav', 'yara', 'volatility', 'rekall', 'wireshark', 'tcpdump', 'ngrep', 'nmap', 'masscan',
            'zmap', 'nessus', 'openvas', 'qualys', 'rapid7', 'burp', 'zap', 'arachni', 'nikto', 'wpscan',
            'joomscan', 'droopescan', 'dirb', 'dirbuster', 'gobuster', 'ffuf', 'wfuzz', 'sqlmap', 'nosqlmap',
            'xsser', 'xsstrike', 'commix', 'tplmap', 'gixy', 'brakeman', 'bandit', 'safety', 'clair', 'trivy',
            'snyk', 'whitesource', 'blackduck', 'veracode', 'checkmarx', 'fortify', 'sonarqube', 'coverity',
            'metasploit', 'canvas', 'coreimpact', 'cobaltstrike', 'empire', 'powershell-empire', 'starkiller',
            'meterpreter', 'mimikatz', 'bloodhound', 'responder', 'impacket', 'hashcat', 'john', 'hydra',
            'medusa', 'patator', 'cewl', 'crunch', 'cupp', 'theharvester', 'sublist3r', 'amass', 'assetfinder',
            'findomain', 'subfinder', 'dnsrecon', 'fierce', 'shodan', 'censys', 'zoomeye', 'binaryedge',
             'intelx', 'leakix', 'netlas', 'onyphe', 'publicwww', 'fofa', 'wigle', 'crt', 'dnsdumpster',
            'virustotal', 'urlscan', 'hybrid-analysis', 'any', 'cuckoo', 'maltego', 'spiderfoot', 'osint-framework',
            'sherlock', 'userrecon', 'holehe', 'social-analyzer', 'ghunt', 'linkedin2username', 'toutatis',
            'twint', 'gallery', 'portfolio', 'resume', 'cv', 'about', 'contact', 'info', 'jobs', 'careers',
            'press', 'media-kit', 'brand', 'styleguide', 'legal', 'privacy', 'terms', 'security', 'trust',
             'abuse', 'dmca', 'copyright', 'sitemap', 'robots', 'humans', 'ads', 'affiliate', 'partner',
            'investor', 'ir', 'relations', 'charity', 'foundation', 'nonprofit', 'volunteer', 'events', 'webinar',
            'podcast', 'newsletter', 'subscribe', 'unsubscribe', 'feedback', 'survey', 'poll', 'quiz', 'game',
            'contest', 'giveaway', 'raffle', 'lottery', 'rewards', 'points', 'coupons', 'deals', 'discounts',
            'sales', 'offers', 'promotions', 'campaigns', 'landing', 'squeeze', 'optin', 'thankyou', 'confirmation',
             'verification', 'activation', 'reset', 'forgot', 'password', 'username', 'account', 'profile',
            'settings', 'preferences', 'notifications', 'messages', 'inbox', 'sent', 'drafts', 'trash', 'spam',
            'contacts', 'friends', 'followers', 'following', 'groups', 'pages', 'likes', 'shares', 'comments',
            'posts', 'articles', 'stories', 'news', 'updates', 'activity', 'feed', 'timeline', 'wall', 'stream',
            'explore', 'discover', 'search', 'find', 'lookup', 'directory', 'map', 'location', 'directions',
             'weather', 'traffic', 'transit', 'flights', 'hotels', 'cars', 'rentals', 'vacations', 'cruises',
            'tours', 'guides', 'tickets', 'reservations', 'bookings', 'orders', 'cart', 'wishlist', 'history',
            'returns', 'shipping', 'tracking', 'delivery', 'invoice', 'receipt', 'bill', 'statement', 'report',
            'summary', 'overview', 'details', 'info', 'faq', 'glossary', 'dictionary', 'encyclopedia', 'wiki',
            'tutorial', 'lesson', 'course', 'class', 'workshop', 'bootcamp', 'degree', 'certificate', 'diploma',
             'transcript', 'syllabus', 'curriculum', 'assignment', 'homework', 'project', 'exam', 'test', 'quiz',
            'grade', 'score', 'result', 'answer', 'solution', 'key', 'cheatsheet', 'reference', 'manual', 'guide'
        ]

    for sub in subs_to_check:
        target = f"https://{sub}.{domain}"
        try:
            response = session.head(target, timeout=timeout, allow_redirects=True)
            if response.status_code < 400 or response.is_redirect:
                print(f"{Colors.GREEN}[+] Found Subdomain: {target} (Status: {response.status_code}){Colors.ENDC}")
                found_subdomains.append(target)
        except requests.exceptions.ConnectionError:
            pass
        except requests.exceptions.RequestException:
            pass

    if not found_subdomains:
        print(f"{Colors.CYAN}[-] No common subdomains found.{Colors.ENDC}")
    return found_subdomains

def generate_base_paths():
    r"""Generates an ultra-deep list of paths for the active scan."""
    print(f"\n{Colors.YELLOW}[*] Generating comprehensive path list for Active Scan...{Colors.ENDC}")

    general = ['admin', 'administrator', 'auth', 'api', 'app', 'assets', 'backup', 'bak', 'beta', 'blog', 'cgi-bin', 'config', 'conf', 'data', 'database', 'db', 'dev', 'development', 'dist', 'docs', 'files', 'git', 'hidden', 'home', 'includes', 'img', 'images', 'import', 'export', 'js', 'json', 'lib', 'local', 'login', 'logs', 'mail', 'media', 'new', 'old', 'phpmyadmin', 'private', 'public', 'scripts', 'secret', 'secure', 'server-status', 'src', 'stage', 'staging', 'static', 'stats', 'support', 'svn', 'system', 'test', 'testing', 'tmp', 'temp', 'tools', 'upload', 'uploads', 'vendor', 'webadmin', 'webmail', 'v1', 'v2', 'v3', 'graphql', 'swagger', 'openapi', 'healthcheck', 'status', 'info', 'metrics', 'prometheus', 'management', 'actuator', 'jolokia', 'heapdump', 'threaddump', 'env', 'configprops', 'beans', 'mappings', 'trace', 'auditevents', 'sessions', 'flyway', 'liquibase', 'caches', 'scheduledtasks', 'httptrace', 'logfile', 'loggers', 'hystrix.stream', 'turbine.stream', 'gateway', 'zuul', 'consul', 'eureka', 'zookeeper', 'kafka', 'rabbitmq', 'redis', 'memcached', 'elasticsearch', 'solr', 'kibana', 'grafana', 'influxdb', 'graphite', 'carbon', 'whisper', 'statsd', 'collectd', 'telegraf', 'nagios', 'icinga', 'zabbix', 'cacti', 'observium', 'librenms', 'smokeping', 'rancid', 'oxidized', 'ansible', 'puppet', 'chef', 'salt', 'terraform', 'vault', 'packer', 'vagrant', 'etcd', 'coreos', 'rancher', 'openshift', 'mesos', 'marathon', 'chronos', 'dcos', 'minikube', 'microk8s', 'k3s', 'eks', 'gke', 'aks', 'ecs', 'fargate', 'lambda', 'functions', 'serverless', 'faas', 'paas', 'iaas', 'saas', 'caas', 'dbass', 'object-storage', 'block-storage', 'file-storage', 'archive-storage', 'glacier', 'deep-archive', 'efs', 'nfs', 'smb', 'cifs', 'iscsi', 'fc', 'fcoe', 'rdp', 'ssh', 'telnet', 'vnc', 'http', 'https', 'dns', 'ntp', 'snmp', 'smtp', 'pop3', 'imap', 'ldap', 'kerberos', 'radius', 'tacacs', 'diameter', 'sip', 'rtp', 'webrtc', 'xmpp', 'mqtt', 'amqp', 'kafka', 'rabbitmq', 'zeromq', 'activemq', 'pulsar', 'nats', 'redis-sentinel', 'redis-cluster', 'memcached', 'hazelcast', 'ignite', 'geode', 'cassandra', 'scylla', 'hbase', 'hadoop', 'spark', 'hive', 'presto', 'trino', 'druid', 'flink', 'storm', 'samza', 'beam', 'airflow', 'luigi', 'argo', 'tekton', 'spinnaker', 'flux', 'helm', 'kustomize', 'istio', 'linkerd', 'envoy', 'traefik', 'nginx', 'apache', 'httpd', 'tomcat', 'jboss', 'wildfly', 'glassfish', 'websphere', 'weblogic', 'iis', 'express', 'koa', 'hapi', 'fastify', 'django', 'flask', 'pyramid', 'tornado', 'rails', 'sinatra', 'hanami', 'laravel', 'symfony', 'codeigniter', 'yii', 'cakephp', 'zend', 'phalcon', 'lumen', 'slim', 'spring', 'springboot', 'quarkus', 'micronaut', 'helidon', 'ktor', 'akka', 'play', 'lift', 'scalyr', 'scalog', 'loggly', 'logz', 'sumologic', 'datadog', 'newrelic', 'appdynamics', 'dynatrace', 'sentry', 'bugsnag', 'rollbar', 'honeycomb', 'lightstep', 'jaeger', 'zipkin', 'opentracing', 'opentelemetry', 'fluentd', 'fluentbit', 'logstash', 'filebeat', 'metricbeat', 'packetbeat', 'heartbeat', 'auditbeat', 'journalbeat', 'winlogbeat', 'osquery', 'wazuh', 'suricata', 'snort', 'bro', 'zeek', 'ossec', 'tripwire', 'aide', 'samhain', 'clamav', 'yara', 'volatility', 'rekall', 'wireshark', 'tcpdump', 'ngrep', 'nmap', 'masscan', 'zmap', 'nessus', 'openvas', 'qualys', 'rapid7', 'burp', 'zap', 'arachni', 'nikto', 'wpscan', 'joomscan', 'droopescan', 'dirb', 'dirbuster', 'gobuster', 'ffuf', 'wfuzz', 'sqlmap', 'nosqlmap', 'xsser', 'xsstrike', 'commix', 'tplmap', 'gixy', 'brakeman', 'bandit', 'safety', 'clair', 'trivy', 'snyk', 'whitesource', 'blackduck', 'veracode', 'checkmarx', 'fortify', 'sonarqube', 'coverity', 'metasploit', 'canvas', 'coreimpact', 'cobaltstrike', 'empire', 'powershell-empire', 'starkiller', 'meterpreter', 'mimikatz', 'bloodhound', 'responder', 'impacket', 'hashcat', 'john', 'hydra', 'medusa', 'patator', 'cewl', 'crunch', 'cupp', 'theharvester', 'sublist3r', 'amass', 'assetfinder', 'findomain', 'subfinder', 'dnsrecon', 'fierce', 'shodan', 'censys', 'zoomeye', 'binaryedge', 'intelx', 'leakix', 'netlas', 'onyphe', 'publicwww', 'fofa', 'wigle', 'crt', 'dnsdumpster', 'virustotal', 'urlscan', 'hybrid-analysis', 'any', 'cuckoo', 'maltego', 'spiderfoot', 'osint-framework', 'sherlock', 'userrecon', 'holehe', 'social-analyzer', 'ghunt', 'linkedin2username', 'toutatis', 'twint', 'gallery', 'portfolio', 'resume', 'cv', 'about', 'contact', 'info', 'jobs', 'careers', 'press', 'media-kit', 'brand', 'styleguide', 'legal', 'privacy', 'terms', 'security', 'trust', 'abuse', 'dmca', 'copyright', 'sitemap', 'robots', 'humans', 'ads', 'affiliate', 'partner', 'investor', 'ir', 'relations', 'charity', 'foundation', 'nonprofit', 'volunteer', 'events', 'webinar', 'podcast', 'newsletter', 'subscribe', 'unsubscribe', 'feedback', 'survey', 'poll', 'quiz', 'game', 'contest', 'giveaway', 'raffle', 'lottery', 'rewards', 'points', 'coupons', 'deals', 'discounts', 'sales', 'offers', 'promotions', 'campaigns', 'landing', 'squeeze', 'optin', 'thankyou', 'confirmation', 'verification', 'activation', 'reset', 'forgot', 'password', 'username', 'account', 'profile', 'settings', 'preferences', 'notifications', 'messages', 'inbox', 'sent', 'drafts', 'trash', 'spam', 'contacts', 'friends', 'followers', 'following', 'groups', 'pages', 'likes', 'shares', 'comments', 'posts', 'articles', 'stories', 'news', 'updates', 'activity', 'feed', 'timeline', 'wall', 'stream', 'explore', 'discover', 'search', 'find', 'lookup', 'directory', 'map', 'location', 'directions', 'weather', 'traffic', 'transit', 'flights', 'hotels', 'cars', 'rentals', 'vacations', 'cruises', 'tours', 'guides', 'tickets', 'reservations', 'bookings', 'orders', 'cart', 'wishlist', 'history', 'returns', 'shipping', 'tracking', 'delivery', 'invoice', 'receipt', 'bill', 'statement', 'report', 'summary', 'overview', 'details', 'info', 'faq', 'glossary', 'dictionary', 'encyclopedia', 'wiki', 'tutorial', 'lesson', 'course', 'class', 'workshop', 'bootcamp', 'degree', 'certificate', 'diploma', 'transcript', 'syllabus', 'curriculum', 'assignment', 'homework', 'project', 'exam', 'test', 'quiz', 'grade', 'score', 'result', 'answer', 'solution', 'key', 'cheatsheet', 'reference', 'manual', 'guide']
    sensitive = ['.env', '.env.local', '.env.dev', 'env.js', '.env.prod', 'config.php', 'config.json', 'settings.py', 'web.config', 'credentials', 'key', 'keys', 'password', 'passwords.txt', 'users.txt', 'id_rsa', 'id_dsa', 'robots.txt', 'sitemap.xml', 'error_log', 'access_log', 'debug.log', 'docker-compose.yml', 'Dockerfile', '.git/config', '.git/HEAD', '.gitignore', '.htaccess', '.htpasswd', 'composer.json', 'package.json', 'yarn.lock', 'security.txt', '.well-known/security.txt']
    cloud_ci = ['metadata', 'aws/credentials', '.circleci/config.yml', 'Jenkinsfile', 'terraform', '.tfstate', 'kubernetes', 'k8s', 'functions', 'lambda', 'webhook', 'gateway', '.serverless', 'serverless.yml']
    tech = ['wp-admin', 'wp-content', 'wp-includes', 'wp-login.php', 'xmlrpc.php', 'user/login', 'CHANGELOG.txt', 'administrator', 'swagger-ui.html', 'actuator', 'actuator/health', 'actuator/env', 'prometheus', 'grafana', 'node_modules']

    base_names = set(general + sensitive + tech + cloud_ci)

    generated_paths = set(base_names)
    print(f"{Colors.GREEN}[+] Generated {len(generated_paths)} unique base paths for active scan.{Colors.ENDC}")
    return list(generated_paths)

def shannon_entropy(data):
    """Calculates the Shannon entropy of a string."""
    if not data:
        return 0
    entropy = 0
    freq = Counter(data)
    data_len = len(data)
    for count in freq.values():
        p_x = count / data_len
        entropy -= p_x * math.log2(p_x)
    return entropy

def analyze_content(status, content, url):
    """Analyzes content for sensitive keys and determines severity, with entropy check."""
    content_lower = content.lower()

    key_patterns = {
        'extremely-critical': [
            re.compile(r'-----BEGIN (?:RSA|OPENSSH|PGP) PRIVATE KEY-----'),
        ],
        'critical': [
            re.compile(r'(?:AWS|aws|Aws)?_?(?:SECRET|secret|Secret)?_?(?:ACCESS|access|Access)?_?(?:KEY|key|Key)[\s:=]+([A-Za-z0-9/+=]{40})'),
            re.compile(r'ghp_[0-9a-zA-Z]{36}'), # Github PAT
            re.compile(r'xox[p|b|o|a]-[0-9]{12}-[0-9]{12}-[0-9]{12}-[a-z0-9]{32}'), # Slack Token
        ],
        'high': [
            re.compile(r'AKIA[0-9A-Z]{16}'), # AWS Key ID (less critical without secret)
            re.compile(r'mongodb(?:\+srv)?:\/\/[^\s"]+'),
            re.compile(r'(sk|pk)_(test|live)_[0-9a-zA-Z]{24}'), # Stripe
            re.compile(r'AIza[0-9A-Za-z\\-_]{35}'), # Google API
            re.compile(r'eyJ[A-Za-z0-9-_=]+\.[A-Za-z0-9-_=]+\.?[A-Za-z0-9-_.+/=]*'), # JWT
        ],
        'medium': [
            re.compile(r'SECRET_KEY|API_KEY|DB_PASSWORD|DATABASE_URL|CLIENT_SECRET', re.IGNORECASE),
        ]
    }

    for severity, patterns in key_patterns.items():
        for pattern in patterns:
            matches = pattern.finditer(content)
            for match in matches:
                value_to_check = match.group(1) if match.groups() else match.group(0)
                entropy = shannon_entropy(value_to_check)
                
                if entropy > 4.0:
                    return 'extremely-critical', f"High Entropy Secret ({pattern.pattern[:20]}...)"
                return severity, f"Leaked Secret ({pattern.pattern[:20]}...)"

    if 'index of' in content_lower or 'directory listing for' in content_lower: return 'medium', "Directory Listing"
    if 'leaked sensitive data' in content_lower: return 'high', "Sensitive Data Leak"
    if 'stack trace' in content_lower or 'exception:' in content_lower or 'sqlstate' in content_lower: return 'medium', "Error/Exception"
    if status in [401, 403]: return 'low', "Forbidden/Unauthorized"
    if status == 200: return 'info', "OK"
    return 'unknown', "Unknown"

def worker(task_queue, session, timeout, results_queue, stop_event, verbose, random_delay, random_agent, method, custom_headers, show_location, show_length):
    r"""The worker thread function that processes tasks from the queue."""
    while not stop_event.is_set():
        try:
            base_url, path = task_queue.get_nowait()
        except queue.Empty:
            break

        if random_delay > 0:
            time.sleep(random.uniform(0, random_delay))

        target_url = urljoin(base_url, path)

        headers = custom_headers.copy()
        if random_agent:
            headers['User-Agent'] = random.choice(USER_AGENTS)

        try:
            response = session.request(method, target_url, timeout=timeout, allow_redirects=False, headers=headers)

            if response.status_code != 404:
                content_length = response.headers.get('Content-Length', 'N/A')
                severity, reason = analyze_content(response.status_code, response.text, target_url)
                location = response.headers.get('Location') if response.is_redirect else None
                results_queue.put(('found', (response.status_code, target_url, content_length, severity, reason, method, location)))
        except requests.exceptions.RequestException as e:
            if verbose:
                with threading.Lock():
                    if "Name or service not known" not in str(e):
                        print(f"{Colors.RED}[!] Network error for {target_url}: {e}{Colors.ENDC}", file=sys.stderr)
            pass
        finally:
            task_queue.task_done()

def strip_colors(text):
    """Removes ANSI color codes from a string."""
    return re.sub(r'\033\[[0-9;?]*[a-zA-Z]', '', text)

def main():
    r"""Main function to parse arguments and run the scan."""

    examples = f"""
{Colors.YELLOW}Examples:{Colors.ENDC}
  {Colors.GREEN}# Full recon: find subdomains, tech, scan dirs, show redirects and content length{Colors.ENDC}
  python3 {sys.argv[0]} -l targets.txt --subdomains --tech-detect --location -cl

  {Colors.GREEN}# Scan a single target with high concurrency and filter for extremely-critical findings only{Colors.ENDC}
  python3 {sys.argv[0]} -u https://example.com -c 100 -s extremely-critical -o critical.txt

  {Colors.GREEN}# Scan with a custom header and a random delay between requests to be stealthy{Colors.ENDC}
  python3 {sys.argv[0]} -u https://internal-app.com -H "Authorization: Bearer <token>" --random-delay 2
    """

    parser = argparse.ArgumentParser(
        description="An advanced, high-performance reconnaissance framework.",
        add_help=False,
        formatter_class=argparse.RawTextHelpFormatter,
        epilog=examples
    )

    req_group = parser.add_argument_group('Target Specification')
    scan_group = parser.add_argument_group('Scan Configuration')
    output_group = parser.add_argument_group('Output & Filtering')
    perf_group = parser.add_argument_group('Performance & Evasion')
    misc_group = parser.add_argument_group('Miscellaneous')

    req_group.add_argument("-u", "--url", help="A single target URL or domain (e.g., example.com).")
    req_group.add_argument("-l", "--list", help="Path to a file containing a list of target URLs/domains.")

    scan_group.add_argument("--subdomains", action="store_true", help="Perform subdomain enumeration.")
    scan_group.add_argument("-w", "--wordlist", help="Custom wordlist file for subdomain enumeration.")
    scan_group.add_argument("--tech-detect", action="store_true", help="Perform server technology detection.")
    scan_group.add_argument("-X", "--method", default="GET", help="HTTP method for directory scan (default: GET).")
    scan_group.add_argument("-H", "--header", action="append", help="Add a custom header (e.g., 'Cookie: session=123').")

    output_group.add_argument("-o", "--output", help="Save the output to a file.")
    output_group.add_argument("--no-color", action="store_true", help="Disable colorized output.")
    output_group.add_argument("-s", "--severity", choices=['info', 'low', 'medium', 'high', 'critical', 'extremely-critical'], help="Filter results by a minimum severity level.")
    output_group.add_argument("--location", action="store_true", help="Display the redirect location if a 3xx status is found.")
    output_group.add_argument("-cl", "--content-length", action="store_true", help="Display the response Content-Length.")


    perf_group.add_argument("-c", "--concurrency", type=int, default=50, help="Number of concurrent tasks (default: 50).")
    perf_group.add_argument("--timeout", type=int, default=10, help="Request timeout in seconds (default: 10).")
    perf_group.add_argument("--random-delay", type=float, default=0, help="Add a random delay (in seconds) between requests.")
    perf_group.add_argument("--no-random-agent", action="store_true", help="Do not use random User-Agents for requests.")


    misc_group.add_argument("-v", "--verbose", action="store_true", help="Enable verbose output for network errors.")
    misc_group.add_argument("-h", "--help", action="help", default=argparse.SUPPRESS, help="Show this help message and exit.")

    args = parser.parse_args()

    if args.no_color or args.output:
        for attr in dir(Colors):
            if not attr.startswith("__"):
                setattr(Colors, attr, Colors.DISABLED)

    print_banner()

    task_queue = queue.Queue()
    results_queue = queue.Queue()
    stop_event = threading.Event()

    targets_from_input = []
    if args.list:
        try:
            with open(args.list, 'r', encoding='utf-8') as f:
                targets_from_input = [line.strip() for line in f if line.strip()]
        except IOError as e:
            print(f"{Colors.RED}[-] Error reading target list file: {e}{Colors.ENDC}", file=sys.stderr)
            sys.exit(1)
    elif args.url:
        targets_from_input.append(args.url)
    else:
        print(f"{Colors.RED}[-] No target specified. Use -u or -l.{Colors.ENDC}", file=sys.stderr)
        parser.print_help()
        sys.exit(1)

    all_domains = {urlparse(t).netloc or t.split('/')[0] for t in targets_from_input}
    targets_to_scan = {f"https://{d}" for d in all_domains}

    severity_order = {'info': 0, 'low': 1, 'medium': 2, 'high': 3, 'critical': 4, 'extremely-critical': 5}
    min_severity = severity_order.get(args.severity, -1)

    custom_headers = {}
    if args.header:
        for header in args.header:
            if ':' not in header:
                print(f"{Colors.RED}[-] Invalid header format: '{header}'. Use 'Key:Value'.{Colors.ENDC}", file=sys.stderr)
                sys.exit(1)
            key, value = header.split(':', 1)
            custom_headers[key.strip()] = value.strip()

    session = requests.Session()
    if not args.no_random_agent:
        session.headers.update({'User-Agent': random.choice(USER_AGENTS)})

    if args.subdomains:
        with requests.Session() as sub_session:
            if not args.no_random_agent:
                sub_session.headers.update({'User-Agent': random.choice(USER_AGENTS)})
            for domain in all_domains:
                found_subs = enumerate_subdomains(domain, sub_session, args.timeout, wordlist_path=args.wordlist)
                targets_to_scan.update(found_subs)

    if args.tech_detect:
        with requests.Session() as tech_session:
             if not args.no_random_agent:
                tech_session.headers.update({'User-Agent': random.choice(USER_AGENTS)})
             for target in sorted(list(targets_to_scan)):
                tech_detect(target, tech_session)

    base_paths = generate_base_paths()

    all_paths = sorted(list(set(base_paths)))
    total_tasks = 0
    for target in targets_to_scan:
        for path in all_paths:
            task_queue.put((target, path))
            total_tasks += 1

    if total_tasks == 0:
        print(f"\n{Colors.YELLOW}[*] No targets or paths to scan. Exiting.{Colors.ENDC}")
        sys.exit(0)

    print(f"\n{Colors.YELLOW}[*] Starting Active Scan on {len(targets_to_scan)} target(s)...{Colors.ENDC}")
    print(f"[*] HTTP Method: {args.method.upper()}")
    print(f"[*] Total Paths to Scan: {total_tasks}")
    print(f"[*] Concurrency: {args.concurrency}{Colors.ENDC}\n")

    start_time = time.time()

    threads = [threading.Thread(target=worker, args=(task_queue, session, args.timeout, results_queue, stop_event, args.verbose, args.random_delay, not args.no_random_agent, args.method, custom_headers, args.location, args.content_length)) for _ in range(args.concurrency)]
    for t in threads: t.start()

    processed_urls = set()
    output_file = None
    if args.output:
        try:
            output_file = open(args.output, 'w', encoding='utf-8')
        except IOError as e:
            print(f"{Colors.RED}[-] Error opening output file: {e}{Colors.ENDC}", file=sys.stderr)
            sys.exit(1)

    try:
        while any(t.is_alive() for t in threads):
            try:
                msg_type, data = results_queue.get(timeout=0.5)
                if msg_type == 'found':
                    status, url, length, severity, reason, method, location = data
                    if url in processed_urls: continue
                    processed_urls.add(url)

                    if min_severity > -1 and severity_order.get(severity, -1) < min_severity: continue

                    output_parts = [f"[{method.upper()}] Found: {url}", f"(Status: {status})"]
                    if args.content_length:
                        output_parts.append(f"(Length: {length})")
                    output_parts.extend([f"(Severity: {severity.upper()})", f"(Reason: {reason})"])
                    
                    if args.location and location:
                        output_parts.append(f"--> {location}")

                    if severity == 'extremely-critical': color = Colors.BOLD_RED
                    elif severity == 'critical': color = Colors.RED
                    elif severity == 'high': color = Colors.ORANGE
                    elif severity == 'medium': color = Colors.YELLOW
                    elif severity == 'low': color = Colors.CYAN
                    else: color = Colors.GREEN

                    console_output = f"{color}{' '.join(output_parts)}{Colors.ENDC}"
                    print(console_output)

                    if output_file:
                        file_output = strip_colors(console_output)
                        output_file.write(file_output + '\n')

            except queue.Empty:
                continue
    except KeyboardInterrupt:
        print(f"\n{Colors.RED}[!] User interrupt detected. Shutting down...{Colors.ENDC}")
        stop_event.set()

    for t in threads: t.join()
    session.close()
    if output_file:
        output_file.close()

    end_time = time.time()
    duration = end_time - start_time
    rps = total_tasks / duration if duration > 0 else 0
    print("\n----------------------------------------------------")
    print(f"{Colors.YELLOW}[*] Scan Finished.{Colors.ENDC}")
    print(f"[*] Total unique paths found: {len(processed_urls)}")
    print(f"[*] Total requests sent: ~{total_tasks}")
    print(f"[*] Time taken: {duration:.2f} seconds")
    print(f"[*] Requests per second: {rps:.2f}")
    if args.output:
        print(f"[*] Results saved to: {args.output}")
    print("----------------------------------------------------")

if __name__ == "__main__":
    main()
